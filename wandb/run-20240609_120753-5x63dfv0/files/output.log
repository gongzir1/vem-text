=> Saving data in Logs/FRL~try=824
dataset to use is:  MNIST
number of FL clients:  1000
non-iid degree data distribution:  0.1
batch size is :  8
test batch size is:  128
10
84
87
98
127
143
179
234
398
400
414
427
473
512
524
534
558
574
582
671
734
767
820
823
876
881
889
921
958
use_cuda:  True
type of FL:  FRL_label_flip_fang
#########Federated Learning using Rankings############
fraction of maliciou clients: 0.20 | total number of malicious clients: 200
e 0 | malicious users: 1 | test acc 0.1134 test loss 2.286013 best test_acc 0.1134
e 1 | malicious users: 4 | test acc 0.2405 test loss 2.246861 best test_acc 0.2405
/home/test/anaconda3/envs/FRL_test/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
e 2 | malicious users: 6 | test acc 0.1154 test loss 2.286202 best test_acc 0.2405
e 3 | malicious users: 6 | test acc 0.1281 test loss 2.245139 best test_acc 0.2405
e 4 | malicious users: 6 | test acc 0.1802 test loss 2.217749 best test_acc 0.2405
